{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KostaKat/MAT442/blob/main/hw3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRJrrTaZzS_V"
      },
      "source": [
        "## 3.7.1 Mathematical Formulation\n",
        "Neural Networks have been a break through in machine learning, and have transformered modern machine learning into deep learning. They model the human brain by having a collection of nodes that are connected, each node is a function.\n",
        "### Simple Neural Network\n",
        "A basic neural network model can be represented as follows:\n",
        "- **Inputs**: $ x_1 $ and $ x_2 $\n",
        "- **Weights**: $ w_1 $ and $ w_2 $\n",
        "- **Bias**: $ b $\n",
        "- **Activation function**: $ \\sigma(z) $, where $ z $ is the weighted sum of inputs plus bias.\n",
        "\n",
        "The output $ \\hat{y} $ is calculated as:\n",
        "$$\n",
        "\\hat{y} = \\sigma(z) = \\sigma(w_1 a_1 + w_2 a_2 + b)\n",
        "$$\n",
        "\n",
        "However, this is just a simple neural network, which is essentially equivalent to linear regression. As a result, its performance will be similar to that of a linear regression model.  However, neural networks can be expanded by stacking more nodes to create deeper networks. These stacked nodes are called layers and by adding layers input data is transformed into multiple linear functions, allowing the model to capture more complex relationships in data. This had found success in multiple applications in both vision and languages tasks, with neural networks having thousands, and even millions of nodes and layers.\n",
        "### Node Calculation in a Layer\n",
        "For node $ j $ in layer $ l $:\n",
        "1. **Weighted Sum**: Calculate the input $ z_j^{(l)} $ to node $ j $ as:\n",
        "\n",
        "\n",
        "$$\n",
        "z_{j'}^{(l)} = \\sum_{j=1}^{J_{l-1}} w_{j,j'}^{(l)} a_{j}^{(l-1)} + b_{j'}^{(l)}\n",
        "$$\n",
        "\n",
        "2. **Activation**: Compute the output $ a_j^{(l)} $ as:\n",
        "   $$\n",
        "   a_j^{(l)} = \\sigma(z_j^{(l)})\n",
        "   $$\n",
        "\n",
        "In matrix form, it is represented as:\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "$$\n",
        "a^{(l)} = \\sigma(z^{(l)})\n",
        "$$\n",
        "\n",
        "- **Note**: The matrix $ W^{(l)} $ contains all weights for layer $ l $, and $ b^{(l)} $ is the bias vector.\n",
        "- The activation function $ \\sigma $ applies element-wise to $ z^{(l)} $.\n",
        "\n",
        "As stated earlier, neural network's nodes essentially perform linear transformation on data. However, this is a bottleneck when trying to capture complex patterns in data. This is were activation function come in. They control the output of each node or neuron and introduce non-linearity to the model.\n",
        "\n",
        "\n",
        "The activation of each layer is computed as:\n",
        "$$\n",
        "a^{(l)} = \\sigma(z^{(l)}) = \\sigma(W^{(l)} a^{(l-1)} + b^{(l)})\n",
        "$$\n",
        "where $ \\sigma $ is the chosen activation function applied element-wise.\n",
        "\n",
        "### Types of Activation Functions\n",
        "\n",
        "1. **Step Function**:\n",
        "   $$\n",
        "   \\sigma(x) = \\begin{cases}\n",
        "      0, & x < 0 \\\\\n",
        "      1, & x \\geq 0\n",
        "   \\end{cases}\n",
        "   $$\n",
        "   The step function is useful for binary classification as it switches on or off at a threshold. However, it is not used in modern networks since its lack of gradient.\n",
        "\n",
        "2. **ReLU (Rectified Linear Unit)**:\n",
        "   $$\n",
        "   \\sigma(x) = \\max(0, x)\n",
        "   $$\n",
        "   ReLU is widely used because it helps combat vanishing gradients. Furthermore, it allows for faster and more effective training on complex networks.\n",
        "\n",
        "3. **Sigmoid Function**:\n",
        "   $$\n",
        "   \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "   $$\n",
        "   The sigmoid function maps inputs to a range between 0 and 1.\n",
        "\n",
        "4. **Softmax Function**:\n",
        "   $$\n",
        "    \\frac{e^{Z_k}}{\\sum_{k=1}^K e^{Z_k}}\n",
        "   $$\n",
        "   The softmax function converts a vector of values into probabilities. This is commonly used in the final layer of multi-class classification networks.\n",
        "\n",
        "### Cost Function\n",
        "\n",
        "The next step in neural networks is cost functions. The cost function measures the error between the neural networkâ€™s predicted output and the actual values from the training data. Two common cost functions are used:\n",
        "\n",
        "1. **Mean Squared Error (MSE)** for regression tasks:\n",
        "   $$\n",
        "   J = \\frac{1}{2} \\sum_{n=1}^N \\sum_{k=1}^K \\left( \\hat{y}_k^{(n)} - y_k^{(n)} \\right)^2\n",
        "   $$\n",
        "\n",
        "2. **Cross-Entropy Loss** for classification tasks, often used for binary classification:\n",
        "  $$\n",
        "J = -\\sum_{n=1}^N \\left( y^{(n)} \\ln(\\hat{y}^{(n)}) + (1 - y^{(n)}) \\ln(1 - \\hat{y}^{(n)}) \\right)\n",
        "$$\n",
        "\n",
        "However, how is a neural network trained? A neural network is trained by backpropagation which minimizes the cost function or error. I works by adjusting layers output based on the previous layer. It used gradient descent on the costfunction to optimize weights (W) and biases (B).\n",
        "\n",
        "\n",
        "#### Key Steps in Backpropagation:\n",
        "1. **Compute Error Gradients**: Using the chain rule, compute partial derivatives of the cost function with respect to weights and biases. Define intermediate terms, $ \\delta_j^{(l)} $, which represent the error in each layer.\n",
        "2. **Iterative Weight Adjustments**: Starting from the output layer, calculate $ \\delta $ values and propagate these back through each layer, updating weights and biases along the way.\n",
        "3. **Update Rules**:\n",
        "   - Weights and biases are updated based on the calculated gradients and a learning rate $ \\beta $.\n",
        "   - New weights and biases are given by:\n",
        "   $$\n",
        "   \\text{New } w_{j,j'}^{(l)} = \\text{Old } w_{j,j'}^{(l)} - \\beta \\frac{\\partial J}{\\partial w_{j,j'}^{(l)}} = \\text{Old } w_{j,j'}^{(l)} - \\beta \\delta_{j'}^{(l)} a_{j}^{(l-1)}\n",
        "   $$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\text{New } b_{j'}^{(l)} = \\text{Old } b_{j'}^{(l)} - \\beta \\frac{\\partial J}{\\partial b_{j'}^{(l)}} = \\text{Old } b_{j'}^{(l)} - \\beta \\delta_{j'}^{(l)}\n",
        "$$\n",
        "\n",
        "4. **Iterate Until Convergence**\n",
        "\n",
        "Source: Textbook\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFicEmEtzS_W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.ToTensor()\n",
        "mnist_train = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert dataset to PyTorch tensors and move to GPU\n",
        "x_train = mnist_train.data.view(-1, 28 * 28).float().to(device) / 255.0\n",
        "y_train = mnist_train.targets.to(device)\n",
        "x_test = mnist_test.data.view(-1, 28 * 28).float().to(device) / 255.0\n",
        "y_test = mnist_test.targets.to(device)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIXyD9StzS_W"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Softmax function for final layer\n",
        "def softmax(z):\n",
        "    exp_z = torch.exp(z - torch.max(z, dim=1, keepdim=True).values)\n",
        "    return exp_z / exp_z.sum(dim=1, keepdim=True)\n",
        "\n",
        "# Cost function (Cross-Entropy Loss)\n",
        "def compute_cost(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -torch.log(y_pred[range(m), y_true] + 1e-8)\n",
        "    cost = torch.sum(log_likelihood) / m\n",
        "    return cost\n",
        "\n",
        "# Network parameters\n",
        "input_size = 784    # 28x28 input images\n",
        "hidden_size = 64    # Size of the hidden layer\n",
        "output_size = 10    # 10 classes for digits 0-9\n",
        "learning_rate = 0.1 # Learning rate for gradient descent\n",
        "\n",
        "# Initialize weights and biases as PyTorch tensors on GPU\n",
        "torch.manual_seed(42)\n",
        "W1 = torch.randn(input_size, hidden_size, device=device) * 0.01\n",
        "b1 = torch.zeros(1, hidden_size, device=device)\n",
        "W2 = torch.randn(hidden_size, output_size, device=device) * 0.01\n",
        "b2 = torch.zeros(1, output_size, device=device)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X):\n",
        "    Z1 = torch.matmul(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = torch.matmul(A1, W2) + b2\n",
        "    A2 = softmax(Z2)  # Softmax applied in the output layer for multiclass classification\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(X, y, Z1, A1, Z2, A2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dZ2 = A2.clone()\n",
        "    dZ2[range(m), y] -= 1\n",
        "    dW2 = torch.matmul(A1.T, dZ2) / m\n",
        "    db2 = torch.sum(dZ2, axis=0, keepdim=True) / m\n",
        "\n",
        "    # Hidden layer error\n",
        "    dA1 = torch.matmul(dZ2, W2.T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
        "    dW1 = torch.matmul(X.T, dZ1) / m\n",
        "    db1 = torch.sum(dZ1, axis=0, keepdim=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVKp-4PCzS_X",
        "outputId": "a290ea9d-bad8-495c-c475-eae4c91738bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost: 2.30229115486145\n",
            "Epoch 100, Cost: 2.2802984714508057\n",
            "Epoch 200, Cost: 2.1132278442382812\n",
            "Epoch 300, Cost: 1.6313488483428955\n",
            "Epoch 400, Cost: 1.211958646774292\n",
            "Epoch 500, Cost: 0.9607114791870117\n",
            "Epoch 600, Cost: 0.8002830743789673\n",
            "Epoch 700, Cost: 0.6926383376121521\n",
            "Epoch 800, Cost: 0.6177592277526855\n",
            "Epoch 900, Cost: 0.5629952549934387\n",
            "Epoch 1000, Cost: 0.5209764838218689\n",
            "Epoch 1100, Cost: 0.4875912666320801\n",
            "Epoch 1200, Cost: 0.46046969294548035\n",
            "Epoch 1300, Cost: 0.4381183087825775\n",
            "Epoch 1400, Cost: 0.41949304938316345\n",
            "Epoch 1500, Cost: 0.4038088321685791\n",
            "Epoch 1600, Cost: 0.3904561996459961\n",
            "Epoch 1700, Cost: 0.37895938754081726\n",
            "Epoch 1800, Cost: 0.36894771456718445\n",
            "Epoch 1900, Cost: 0.36013373732566833\n",
            "Epoch 2000, Cost: 0.3522941470146179\n",
            "Epoch 2100, Cost: 0.3452552855014801\n",
            "Epoch 2200, Cost: 0.3388809859752655\n",
            "Epoch 2300, Cost: 0.3330637812614441\n",
            "Epoch 2400, Cost: 0.3277179002761841\n",
            "Epoch 2500, Cost: 0.3227742910385132\n",
            "Epoch 2600, Cost: 0.31817662715911865\n",
            "Epoch 2700, Cost: 0.3138786852359772\n",
            "Epoch 2800, Cost: 0.309842050075531\n",
            "Epoch 2900, Cost: 0.30603450536727905\n",
            "Epoch 3000, Cost: 0.3024289608001709\n",
            "Epoch 3100, Cost: 0.2990024983882904\n",
            "Epoch 3200, Cost: 0.29573535919189453\n",
            "Epoch 3300, Cost: 0.29261064529418945\n",
            "Epoch 3400, Cost: 0.28961384296417236\n",
            "Epoch 3500, Cost: 0.2867322862148285\n",
            "Epoch 3600, Cost: 0.28395506739616394\n",
            "Epoch 3700, Cost: 0.2812725007534027\n",
            "Epoch 3800, Cost: 0.27867624163627625\n",
            "Epoch 3900, Cost: 0.27615898847579956\n",
            "Epoch 4000, Cost: 0.27371418476104736\n",
            "Epoch 4100, Cost: 0.27133625745773315\n",
            "Epoch 4200, Cost: 0.2690200209617615\n",
            "Epoch 4300, Cost: 0.2667611539363861\n",
            "Epoch 4400, Cost: 0.2645556926727295\n",
            "Epoch 4500, Cost: 0.2624001204967499\n",
            "Epoch 4600, Cost: 0.26029136776924133\n",
            "Epoch 4700, Cost: 0.2582266628742218\n",
            "Epoch 4800, Cost: 0.2562035620212555\n",
            "Epoch 4900, Cost: 0.25421983003616333\n",
            "Test Accuracy: 92.82%\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    Z1, A1, Z2, A2 = forward_propagation(x_train)\n",
        "\n",
        "    # Compute cost\n",
        "    cost = compute_cost(y_train, A2)\n",
        "\n",
        "    # Backpropagation\n",
        "    dW1, db1, dW2, db2 = backpropagation(x_train, y_train, Z1, A1, Z2, A2)\n",
        "\n",
        "    # Gradient descent update\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    # Print the cost every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Cost: {cost.item()}\")\n",
        "\n",
        "# Prediction function\n",
        "def predict(X):\n",
        "    _, _, _, A2 = forward_propagation(X)\n",
        "    return torch.argmax(A2, axis=1)\n",
        "\n",
        "# Evaluate accuracy on the test set\n",
        "predictions = predict(x_test)\n",
        "accuracy = torch.mean((predictions == y_test).float()) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}