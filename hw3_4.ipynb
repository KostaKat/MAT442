{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KostaKat/MAT442/blob/main/hw3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PJyKXCVrbhm"
      },
      "source": [
        "\n",
        "## Logistic Regression\n",
        "\n",
        "Logistic regression is a model that uses a **logistic function** to perform binary classification, and can also be extended to multi-class classification.\n",
        "\n",
        "\n",
        "Given samples $ \\{(\\alpha_i, b_i) : i = 1, \\dots, n\\} $ , where:\n",
        "- $ \\alpha_i \\in \\mathbb{R}^d $ are the **features** (a vector),\n",
        "- $ b_i \\in \\{0, 1\\} $ is the **label** (binary).\n",
        "\n",
        "Given the input samples we define a matrix $ A \\in \\mathbb{R}^{n \\times d} $ with rows $ \\alpha_i^T $ for $ i = 1, \\dots, n $, and a label vector $ b = (b_1, \\dots, b_n)^T \\in \\{0, 1\\}^n $.\n",
        "\n",
        "The goal of logistic regression is to **approximate the probability** of a sample having a label of 1. This is done through the **logit function**, which expresses the relationship between the features of a sample and the probability of the label being 1 as a linear function. This is defined to be a regression problem and is formally defined as:\n",
        "$$\n",
        "p(\\alpha;x) = \\log \\left( \\frac{p(\\alpha ; x)}{1 - p(\\alpha ; x)} \\right) = \\alpha^T x\n",
        "$$\n",
        "Where $ x \\in \\mathbb{R}^d $ are the model parameters.\n",
        "\n",
        "\n",
        "Thus, the probability $ p(\\alpha) $ is given by:\n",
        "$$\n",
        "p(\\alpha; x) = \\sigma(\\alpha^T x) = \\frac{1}{1 + e^{-\\alpha^T x}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To estimate the model parameters, we maximize the **likelihood** of the data:\n",
        "$$\n",
        "\\mathcal{L}(x; A, b) = \\prod_{i=1}^n p(\\alpha_i ; x)^{b_i} (1 - p(\\alpha_i ; x))^{1 - b_i}\n",
        "$$\n",
        "To make the process more efficient, applications use the negative log-likelihood (also known as the **cross-entropy loss**) is:\n",
        "$$\n",
        "\\ell(x; A, b) = - \\frac{1}{n} \\sum_{i=1}^n b_i \\log(\\sigma(\\alpha^T x)) - \\frac{1}{n} \\sum_{i=1}^n (1 - b_i) \\log(1 - \\sigma(\\alpha^T x))\n",
        "$$\n",
        "\n",
        "\n",
        "Ultimately making it a minimization problem, that minimizes the cross-entropy loss.\n",
        "\n",
        "It is apparent that various approaches can be taken. For example, taking the gradient of the loss function, which will give us the global minimum. However, as discussed in the previous section, this approach is inefficient, especially for large datasets. Thus,  we use **stochastic gradient descent**, which is formally expressed as:\n",
        "$$\n",
        "x^{k+1} = x^k + \\beta \\left( b_i - \\sigma(\\alpha_i^T x^k) \\right) \\alpha_i\n",
        "$$\n",
        "where $ i $ is selected randomly from $ \\{1, \\dots, n\\} $ and $ \\beta $ is the step size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwB59bpQrbhn",
        "outputId": "8cdfca83-3e9b-4a72-eeb4-66f1d8a70b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.6744941702980122\n",
            "Epoch 10, Loss: 0.661699873896178\n",
            "Epoch 20, Loss: 0.6793574595548757\n",
            "Epoch 30, Loss: 0.5611247510844308\n",
            "Epoch 40, Loss: 0.5622386349398653\n",
            "Epoch 50, Loss: 0.5021251717517545\n",
            "Epoch 60, Loss: 0.49621516913357183\n",
            "Epoch 70, Loss: 0.4608039882151945\n",
            "Epoch 80, Loss: 0.46508414665590614\n",
            "Epoch 90, Loss: 0.4115264397943958\n",
            "\n",
            "Accuracy on the test set: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Cross-entropy loss\n",
        "def compute_loss(A, b, x):\n",
        "    n = len(b)\n",
        "    predictions = sigmoid(np.dot(A, x))\n",
        "    loss = - (1 / n) * (np.dot(b, np.log(predictions)) + np.dot((1 - b), np.log(1 - predictions)))\n",
        "    return loss\n",
        "\n",
        "# Stochastic Gradient Descent (SGD)\n",
        "def stochastic_gradient_descent(A, b, x_init, learning_rate, epochs):\n",
        "    n, d = A.shape\n",
        "    x = x_init.copy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Pick a random sample\n",
        "        i = np.random.randint(n)\n",
        "\n",
        "        # Compute gradient for the selected sample\n",
        "        gradient = (b[i] - sigmoid(np.dot(A[i], x))) * A[i]\n",
        "\n",
        "        # Update rule for stochastic gradient descent\n",
        "        x = x + learning_rate * gradient\n",
        "\n",
        "        # print loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            loss = compute_loss(A, b, x)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return x\n",
        "\n",
        "# Prediction function\n",
        "def predict(A, x):\n",
        "    probabilities = sigmoid(np.dot(A, x))\n",
        "    return (probabilities >= 0.5).astype(int)\n",
        "\n",
        "# Load the Iris dataset and use only two classes\n",
        "iris = datasets.load_iris()\n",
        "A = iris.data[iris.target != 2]  # Use only setosa and versicolor\n",
        "b = iris.target[iris.target != 2]  # Setosa is class 0 and versicolor is class 1\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize parameters for logistic regression\n",
        "x_init = np.zeros(A_train.shape[1])\n",
        "\n",
        "# train model\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "learned_x = stochastic_gradient_descent(A_train, b_train, x_init, learning_rate, epochs)\n",
        "\n",
        "# test model\n",
        "b_pred = predict(A_test, learned_x)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(b_test, b_pred)\n",
        "print(f\"\\nAccuracy on the test set: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}